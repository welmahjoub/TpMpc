# Chargement des librairies n√©c√©ssaires
library(stats)
library(expsmooth)
library(forecast)

########## Premi√®re m√©thode ######################
####   R√©gression lin√©aire sur tout le pass√©  series avec tendance !!!####
############################################led_best#####
 
# Chargement de la s√©rie DAX

dax = read.table("./dax.txt", header=  T)

# L'option header = T (true) signifie qu'il y a des en-t√™tes
# pour les noms de colonne dans le fichier txt

# Inspection du d√©but de la s√©rie

head(dax)#Valeurs

# Cela vous donne les 6 premi√®res valeurs de la s√©rie.

# Question : Comment s'appele la colonne qui contient les valeurs de la s√©rie ?
Valeurs

# Question : Combien de points (ou de valeurs) comporte la s√©rie dax?

1841

# Question : Afficher la s√©rie √† l'aide de la commande plot. Choisissez le type
# de ligne qui vous semble le plus adapt√©

plot(dax$Valeurs, typ = "b")

# Question :
# En regardant le graphe de la s√©rie, pensez-vous qu'il y ait une tendance
# et/ou une saisonnalit√©?

oui tendance mais pas de saisonnalite

# Question : Tracer le corr√©logramme de la s√©rie dax.

acf(dax, lag.max = 1841)

# Quelle conclusion pouvez-vous tirer ?

tendance car long temps les valeurs sont positive


# Vous allez maintenant appliquer les m√©thodes 
# vues en cours pour faire de la pr√©diction avec tendance mais sans saisonnalit√©, c‚Äôest √† dire :

#  ‚Äî pr√©vision par r√©gression lin√©aire sur toutes les valeurs pass√©es
#  ‚Äî pr√©vision par r√©gression linaire sur les k derni√®res valeurs
#  ‚Äî pr√©vision par lissage exponentiel double

###### 1√®re m√©thode : pr√©diction par r√©gression lin√©aire sur toutes les valeurs pass√©es:

### On commence par une tendance lin√©aire (une droite), m√™me si cela n'a pas l'air
# adapt√© √† notre s√©rie

# La variable pr√©dictrice est la variable qui repr√©sente le temps : c'est un 
# vecteur qui comporte les valeurs 1,2,3, ..., jusqu'au nombre de valeurs de la s√©rie

t = c(1:length(dax$Valeurs))# vecteur de temps 

#Le mod√®le de r√©gression que l'on construit tente d'expliquer la s√©rie dax$Valeurs
# en fonction de t:

modele1= lm(dax$Valeurs~t) #modele lineaire
modele1#955.085        1.675  


# Ce mod√®le √©tant une droite qui mod√©lise dax en fonction du temps, son √©quation est donc:

# dax = a + b*t

# Les valeurs de a et b sont les coefficients du mod√®le et sont donn√©s par 

modele1$coefficients

# Question = Quelle est l'√©quation de la droite que vous venez d'estimer ?

dax=955.084893 + 1.674871 t

# Affichage de la s√©rie, ainsi que l'estimation de sa tendance 

plot(dax$Valeurs, typ = "l")

abline(modele1, col = "red")# afficher la droite du modele lineaire

# Question : Ce mod√®le semble t'il adapt√© ?

no car il passe pas par tous les points

# La derni√®re obsevation que l'on a de cette s√©rie est celle de l'instant t=1841.

# Si on veux pr√©dire le prochain point de cette s√©rie, il faut pr√©dire l'instant t = 1842.
# Pour cela, il faut cr√©er une data frame contenant la valeur de t pour laquelle
# on souhaite pr√©dire les valeurs de dax et indiquer le nom de la variable
# explicative (ici t)

newdata = data.frame("t"=1842) # l'instant pour lequel on veut pr√©dire 
prediction = predict(modele1, newdata) # on fait la pr√©diction
prediction#4040.198 : pour l'instant 1842 ( prochaine valeur du serie ) 


# Question : D'apr√®s l'√©quation du mod√®le, quelle doit √™tre la pr√©diction √† l'instant t=1842 ?

v=955.084893 + (1.674871 *1842 )

4040.197

# V√©rifiez que cette valeur est bien celle donn√©e par la commande predict

# Affichage de la s√©rie et de la pr√©diction que l'on vient de faire

plot(dax$Valeurs, typ = "l", xlim = c(1,1842), ylim = c(1500,6200))# affiche serie 

lines(modele1$fitted.values, col = "red")# afficher modele lineaire (droite)

points(c(1842),4040.198, col = "blue", pch = 3)# afficher la valeur de prediction du point 1842 sous forme de +

modele1$fitted.values# afficher tous les prediction de la series faite par le modele lineaire


# Pour pr√©dire plusieurs points suivants, il faut suivre la m√™me d√©marche:

newdata = data.frame("t"=c(1842:1900)) # si on veut les 59 suivants par exemple
prediction = predict(modele1, newdata) # on fait la pr√©diction
prediction

# Question : Afficher la s√©rie et les pr√©dictions que l'on vient de faire

plot(dax$Valeurs, typ = "l", xlim = c(1,1900), ylim = c(1500,6200))# afficher serie
lines(modele1$fitted.values, col = "red")# afficher model
points(c(1842:1900),prediction, col = "blue", pch = 3)# afficher les prediction


# On voudrait maintenant estimer la performance de cette m√©thode (regr√©ssion lin√©aire sur tout le pass√©)
# sur ces donn√©es. Pour cela, il faut faire des pr√©dictions pour des points que l'on connait, afin
# de calculer ensuite un erreur quadratique.

# Question : en vous aidant des commandes ci-dessus, cr√©er un mod√®le pour pr√©dire la derni√®re valeur
# de la s√©rie dax (i.e. pour t = 1841). Attention, le point pour t = 1841 ne doit pas √™tre donn√© pour
# apprendre la droite de r√©gression.
# Pr√©dire la derni√®re valeur de la s√©rie avec ce mod√®le et calculer l'EQ
#-2145.892 


#prediction du valeur 1841
fin=length(dax$Valeurs)-2
t = c(1:fin)# vecteur de temps 

modele1= lm(dax$Valeurs[1:fin]~t) #modele lineaire

newdata = data.frame("t"=1841) # si on veut les 59 suivants par exemple
prediction = predict(modele1, newdata) # on fait la pr√©diction
prediction#4038.523 

# calculer l'erreur 

diff= prediction - dax$Valeurs[1841]

diff# -2147.567 

#eqm 
diff ^2 #4612044


# On peut refaire √ßa pour t = 1840, 1839, 1838, ....
# De cette fa√ßon, on va avoir beaucoup d'estimation des erreurs faites par le mod√®le.
# On pourra donc mieux estimer sa performance globale.
# On va maintenant faire √ßa de fa√ßon un peu plus automatique : 
# Question : √©crire (dans le fichier fonctions_tp1_st.R), une fonction 
# prediction_lineaire(s, i)
# qui prend en param√®tre une s√©rie s, et pr√©dit la i√®me valeur de cette s√©rie par la m√©thode 
# de r√©gression lin√©aire sur tout le pass√© (c'est √† dire avant i strictement)

 predictionLm=function(s,x)
{
	fin=x-1
	t = c(1:fin)# vecteur de temps 

	modele1= lm(s[1:fin]~t)# modele lineaire
	newdata = data.frame("t"=x) # l'instant pour lequel on veut pr√É¬©dire 
	prediction = predict(modele1, newdata) # on fait la pr√É¬©diction

	return (prediction)
}

 predictionLm(dax$Valeurs,1841)
#4038.523 

# Question : √† l'aide de la commande sapply() (idem TP 1), cr√©er un vecteur qui contient les 
# pr√©dictions aux instants t = 5,6,7, ... , 1841 par cette m√©thode.
# Puis calculer l'EQM de ces pr√©dictions. Attention √† bien comparer une pr√©diction avec la valeur 
# de la s√©rie √† l'instant correspondant.

source("./fonctions_tp1_st.R")

prediction=sapply(c(5:1841), function(x){
	predictionLm(dax$Valeurs,x)
})


eqmpredictionlinneaire = eqm(prediction,dax$Valeurs[5:1841])

eqmpredictionlinneaire #289944.4

# Retenez cette EQM, vous la comparerez avec la performance des autres m√©thodes ensuite.

##### On va maintenant estimer la tendance par un polynome de degr√© 2:

modele2 = lm(dax$Valeurs~poly(t,2, raw=T))# generer modele polynomiale de degree 2

# maintenant on a deux variables explicatives : t et t^2 (que l'on doit noter I(t^2))
# Le mod√®le estim√© est donc un polynome de degr√© 2 : 

# dax = a + b*t + c*t^2

# Les coefficients du mod√®le sont donn√©s par 

modele2$coefficients

1.956e+03            -1.584e+00             1.769e-03 

# Question : Quelle est l'√©quation du mod√®le ?

dax = 1.956e+03 + -1.584e+00*t + 1.769e-03 +t^2

# Affichage de la s√©rie et du mod√®le:

plot(dax$Valeurs, typ = 'l')
lines(modele2$fitted.values, col = "red")

# Est ce que √ßa semble convenir ?
Oui Áa semble convenir

# Pour pr√©dire la (ou les) prochaine(s) valeur(s) de la s√©rie, il faut 
# utiliser comme tout √† l'heure un data.frame contenant les valeurs des instants
# pour lesquels on souhaite pr√©dire, et ensuite utiliser la fonction predict


# Question : Afficher sur un m√™me graphique, la s√©rie, le polyn√¥me que vous
# avez cr√©√© et les 19 prochaines pr√©dictions faites par ce polyn√¥me.

newdata = data.frame("t"=c(1842:1861)) # si on veut les 19 suivants par exemple
prediction = predict(modele2, newdata) # on fait la pr√©diction
prediction

plot(dax$Valeurs, typ = 'l')
lines(modele2$fitted.values, col = "red")

points(c(1842:1861),prediction, col = "blue", pch = 3)

# Question : Refaites la m√™me proc√©dure pour un polyn√¥me de degr√© 3, puis 5, 
# puis d'autres valeurs encore plus grandes. Afficher √† chaque fois
# la s√©rie, le polyn√¥me et les pr√©dictions.
# Qu'observez-vous ?

#DegrÈs 3 :

modeled3 = lm(dax$Valeurs~poly(t,3, raw=T))

newdata = data.frame("t"=c(1842:1900)) # si on veut les 59 suivants par exemple
prediction = predict(modeled3, newdata) # on fait la pr√©diction
prediction

plot(dax$Valeurs, typ = 'l')
lines(modeled3$fitted.values, col = "red")
points(c(1842:1900),prediction, col = "blue", pch = 3)


#DÈgrÈs 5:

modeled5 = lm(dax$Valeurs~poly(t,5, raw=T))
newdata = data.frame("t"=c(1842:1900)) # si on veut les 59 suivants par exemple
prediction = predict(modeled5, newdata) # on fait la pr√©diction
prediction

plot(dax$Valeurs, typ = 'l')
lines(modeled5$fitted.values, col = "red")
points(c(1842:1900),prediction, col = "blue", pch = 3)

On observe que le dÈgrÈs 5 est plus adaptÈ pour explier la serie

## Maintenant, vous allez effectuer une d√©marche pour estimer la performance
# d'un pr√©diceur de type polynomial de degr√© d (param√®tre)
# Pour cela,
# 1) √©crire une fonction prediction_polynomiale(s,i,d) qui pr√©dit la
# i√®me valeur de la s√©rie s avec un polynome de degr√© d
# 2) Calculer les errreurs de pr√©diction pour diff√©rentes valeurs de i (comme pr√©c√©demment)
# 3) Calculer l'EQM de toutes ces erreurs

prediction_polynomiale= function(s,i,d){
	
	fin=i-1
	t = c(1:fin)
	modele = lm(s[1:fin]~ poly(t,d, raw=T))
	newdata = data.frame("t"=i) 
	prediction = predict(modele, newdata) # on fait la pr√É¬©diction
	return (prediction)

}

source("./fonctions_tp1_st.R")

prediction_polynomiale(dax$Valeurs,1841,8)#6212.713 : degre 8 de 1841

# predire tous les valeurs

prediction=sapply(c(5:1841), function(x){
	prediction_polynomiale(dax$Valeurs,x,5)
})

# les erreur 
erreurd5= erreur(prediction, dax$Valeurs[5:1841])

# eqm 
eqmd5 = eqm(prediction, dax$Valeurs[5:1841])
eqmd5 #24911.38

# Ensuite, tester diff√©rentes valeurs de d, et choisir celle qui vous semble la meilleure


# prediction  k=2

prediction=sapply(c(5:1841), function(x){
	prediction_polynomiale(dax$Valeurs,x,2)
})

eqmd2 = eqm(prediction, dax$Valeurs[5:1841])
eqmd2 #89584.19


# prediction  k=8

prediction=sapply(c(5:1841), function(x){
	prediction_polynomiale(dax$Valeurs,x,8)
})

eqmd8 = eqm(prediction, dax$Valeurs[5:1841])
eqmd8 # 19630.18

# prediction  k=3

prediction=sapply(c(5:1841), function(x){
	prediction_polynomiale(dax$Valeurs,x,3)
})

erreurd3= erreur(prediction, dax$Valeurs[5:1841])

eqmd3 = eqm(prediction, dax$Valeurs[5:1841])
eqmd3 #36472.16


L'eerreur du dÈgrÈs 8 est plus petit donc on le garde

###### 2√®me m√©thode : pr√©diction par regression sur les k derni√®res valeurs


# Refaites la m√™me analyse, mais cette fois-ci en utilisant la m√©thode
# de pr√©diction par r√©gression sur les k derni√®res valeurs.
# C'est tr√®s similaire √† ce que vous venez de faire, sauf qu'au lieu 
# de prendre tout le pass√© en consid√©ration, vous ne devez prendre que
# les k derni√®res valeurs. 

# Attention, la variable explicative est toujours le temps, mais cette fois-ci,
# on ne d√©marre pas √† t=1. On d√©marre √† t=length(s) - k + 1 (seulement les k derniers instants) 

# Le plus pratique est de cr√©er une fonction 
# prediction_tendance_k_derniere(s, k, i, d)
# avec s une serie
# k le parametre pour r√®gler le nombre de derni√®res valeurs qu'on veut garder
# i l'instant pour lequel on veut pr√©dire
# d le degr√© du polynome pour estimer la tendance
# et qui renvoie la pr√©diction de la i√®me valeur de la s√©rie

# Ensuite, d√©terminer quelles valeurs de k et de d semblent les meilleures pour
# cette s√©rie

#debut=(length(s) - k ) + 1 
#debut=(i - k ) + 1 

prediction_tendance_k_derniere=function(s, k, i, d)
{
	debut=(i - k) 
	fin = (i-1)
	t = c(debut:fin)

	modele = lm(s[debut:fin]~poly(t,d, raw=T))

	newdata = data.frame("t"=i) 
	prediction = predict(modele, newdata) # on fait la pr√É¬©diction
	return (prediction)

}

source("./fonctions_tp1_st.R")

# pour k=5 et degre 5

prediction_tendance_k_derniere(dax$Valeurs, 1, 1840, 8)#6108  

# prediction pour tous la serie  avec k=2 et degre=8

prediction=sapply(c(5:1841), function(x){
	prediction_tendance_k_derniere(dax$Valeurs, 2, x, 8)
})

eqmK = eqm(prediction, dax$Valeurs[5:1841])
eqmK #1956.905


###### 3√®me m√©thode : lissage exponentiel double (LED)

# Le LED est tr√®s facile d'utilisation avec R, il faut utiliser la commande HoltWinters de la fa√ßon suivante:

led = HoltWinters(dax$Valeurs, alpha = 0.5, beta = 0.5, gamma = F)
led

# a 6183.94834
# b   29.99818

# Il faut donner comme param√®tres : 
# - 1 s√©rie temporelle (ici c'est dax$Valeurs)
# - une valeur pour le param√®tre alpha du LED (ici on met 0.5, on essaiera de changer plus tard)
# - une valeur pour le param√®tre beta du LED (ici 0.5 pour commencer) 
# - et gamma = F (comme false), car on n'en a pas besoin en LED

# Question : 

# Afficher le mod√®le obtenu, en tapant led (le nom de la variable qui stocke le mod√®le) 

led

# Vous devez voir que alpha et beta sont bien √† 0.5,  et gamma n'est pas utilis√©

# Vous devez √©galement avoir une autre r√©sultat affich√©: "Coefficients".

# Ce vecteur contient 2 valeurs a et b qui sont les 2 r√©sultats issus du LED (cf cours, 
# ce sont les derniers a et b calcul√©s)

# On peut r√©cup√©rer ces valeurs en tapant

led$coefficients

 6183.94834  29.99818 

# Question : Comment peut-on pr√©dire (gr√¢ce √† a et b) la prochaine valeur de la s√©rie (instant t=1842) ? (cf cours)
# Et la valeur suivante (t=1843) ? 

#prediction pour linstant t=1842 :
6183.94834 +29.99818 #6213.947

#prediction pour linstant t=1843 : a + (b*2) 
# 2 : nombre d'annes apres la fin du serie 
6183.94834 + (2*29.99818 )# 6243.945

# La commande predict permet faire les pr√©dicitions des prochaines valeurs en utilisant le mod√®le obtenu:

predict(led, 1)# 6213.947 : ( t=1842) prochaine valeur
predict(led, 2)#6243.945  : t=1843

# - led correspond au mod√®le qu'on a estim√©
# - et le 2 car je demande les 2 prochaines pr√©dictions ici par exemple.

# Question:
# V√©rifier que les pr√©dictions sont bien celles attendues.

# Si vous vous souvenez du fonctionnement du LED, deux coefficients a et b sont 
# estim√©s √† chaque instant de la s√©rie, √† partir de t = 3. On les a nomm√©s a_t et b_t en cours
# On peut retrouver toutes ces valeurs dans la matrice:

led$fitted

# Les lignes de cette matrice sont les instants temporels t = 3, t = 4, etc..., jusque t = 1841 (dernier
# instant de la s√©rie)

# La premi√®re colonne donne x^_t, c'est √† dire la pr√©diction du t-i√®me √©l√©ment de la s√©rie
# La deuxi√®me colonne donne a_(t-1), c'est √† dire le coefficient a (level) de l'instant pr√©c√©dent
# La troisi√®me colonne donne b_(t-1), c'est √† dire le coefficient b (Trend) de l'instant pr√©c√©dent

# Vous pouvez v√©rifier que x^_t = a_(t-1) + b_(t-1) comme indiqu√© dans le cours.

# prediction pour t=1840 ( a1839 +b1839)

x1840= 6118.962 + 2.393701e+01
x1840#6142.899

# Donc vous pouvez r√©cup√©rer toutes les pr√©dictions interm√©diaires x^_t pour t = 3, 4, ...1841 
# par la commande:

led$fitted[,1] # recuere 1 colonne qui correspond au valeurs de prediction

# Question : 
# Calculer l'EQM de toutes ces pr√©dictions.
# Attention, la premi√®re pr√©diction dans led$fitted correspond √† t = 3. Il faut donc la 
# comparer √† la 3√®me valeur de la s√©rie. etc...

prediction=led$fitted[,1]# prediction de tous la serie (3 ... 181 )

eqmTed = eqm(prediction, dax$Valeurs[3:1841])
eqmTed #1485.494

# Question : 
# Refaites les m√™mes op√©rations mais en choisissant un autre couple de valeurs alpha, beta (au hasard)
# Comparez les EQM obtenues

# Si vous ne donnez pas de valeurs de alpha et beta dans l'appel √† la fonction led, cette fonction 
# va automatiquement s√©lectionner le couple pour lequel l'EQM est minimale.

led_best= HoltWinters(dax$Valeurs, gamma = F)

led_best

# Question : 
# Quels sont les valeurs de alpha, beta obtenues ?

 alpha: 0.9893016
 beta : 0.01158487
 
# a et b sont les derniere valeur ( a et b ) calculer => a et b du 1841

# Calculer l'EQM associ√©e. Elle doit √™tre inf√©rieure √† celles que vous avez obtenues avec les pr√©c√©dents led

prediction=led_best$fitted[,1]
eqmTed = eqm(prediction, dax$Valeurs[3:1841])
eqmTed #978.8969

#### Conclusion : Choix du meilleur mod√®le pour pr√©dire la s√©rie dax
	Le meilleur modËle est le lissage exponnentiel Double

# Parmi toutes les m√©thodes que vous avez essay√©es, quelle est celle qui conduit √† la plus petite EQM ?
	Le lissage exponentielle double

# Appliquer cette m√©thode pour pr√©dire les 50 prochaines valeurs de la s√©rie, et afficher les pr√©dictions.

predict(led_best, 50)

